{"cells":[{"cell_type":"markdown","source":["# Fabric Lakehouse Replicator\n","\n","\n","## This notebook clones lakehouses and their tables (including data) across Fabric workspaces.\n","\n","###### Author: [Rui Cunha]\n","###### GitHub: [https://github.com/ruippcunha/msfabric_samples]\n","\n","#### Disclaimer\n","\n","This notebook is provided as a sample implementation intended for development and testing purposes only. It is not designed or recommended for use in production environments.\n","Although this notebook was developed by a Microsoft employee, it is not an officially supported solution, use it  at your own risk. . Microsoft and the author assume no responsibility for any issues or data loss that may result from its use in live systems.\n","Before applying any part of this solution in production, ensure it is thoroughly reviewed, tested, and adapted to meet your organization‚Äôs security, compliance, and operational requirements.\n","\n","Please note that replicating Lakehouse data across workspaces may result in additional storage costs.\n","You can find more information about Microsoft Fabric storage pricing here:[Microsoft Fabric Pricing](https://azure.microsoft.com/en-us/pricing/details/microsoft-fabric/)\n","\n"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"3152b9f2-a276-4614-ab45-9c22cbb65c0f"},{"cell_type":"code","source":["#toggle this as a parameter cell\n","\n","# Define the source workspace ID from where you want to replicate the Lakehouse(s)\n","source_workspace_id = \"6d421ea5-43d4-47c6-a0f4-99eb71d8dd50\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"7d830960-3bb9-4a1f-b898-dfb222c8534a","normalized_state":"finished","queued_time":"2025-07-11T10:55:54.1298008Z","session_start_time":null,"execution_start_time":"2025-07-11T10:55:54.1309797Z","execution_finish_time":"2025-07-11T10:55:54.4729266Z","parent_msg_id":"243d584e-7293-49b1-a454-e498358255cc"},"text/plain":"StatementMeta(, 7d830960-3bb9-4a1f-b898-dfb222c8534a, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7590c9b8-da6f-4fca-a0b1-26e21e9cc79f"},{"cell_type":"code","source":["import time\n","import requests\n","import json\n","from notebookutils import mssparkutils\n","import re\n","import sempy.fabric as fabric \n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import lit\n","# -------------------------------\n","# Parameters\n","# -------------------------------\n","#source_workspace_id = {source_workspace_id}\n","if not source_workspace_id:\n","    raise ValueError(\"‚ùå 'source_workspace_id' parameter is required.\")\n","# -------------------------------\n","# Get Current Workspace ID\n","# -------------------------------\n","print(\"Getting the workspace ID\")\n","client = fabric.FabricRestClient()\n","workspace_id = fabric.get_notebook_workspace_id()\n","print(f\"Current workspace ID: {workspace_id}\")\n","\n","print(f\"Replicating all lakehouses from Workspace {source_workspace_id} to this Workspace with id {workspace_id}\")\n","source_workspace_id = \"6d421ea5-43d4-47c6-a0f4-99eb71d8dd50\" #workspace where the source LH exists \n","# -------------------------------\n","# Authentication\n","# -------------------------------\n","token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","headers = {\n","    \"Authorization\": f\"Bearer {token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","# -------------------------------\n","# List Lakehouses in Source Workspace\n","# -------------------------------\n","print(f\"Listing all Lakehouses in workspace {source_workspace_id}\")\n","lakehouse_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{source_workspace_id}/items?type=Lakehouse\"\n","response = requests.get(lakehouse_url, headers=headers)\n","if response.status_code == 200:\n","    lakehouses = response.json().get(\"value\", [])\n","    print(f\"Found {len(lakehouses)} lakehouses in workspace {source_workspace_id}:\\n\")\n","# -------------------------------\n","# Process Each Lakehouse\n","# -------------------------------\n","    for lh in lakehouses:\n","        lh_id = lh['id']\n","        lh_name = lh['displayName']\n","        print(f\"\\nüìò Processing Lakehouse: {lh_name} (ID: {lh_id})\")\n","        # Step 3.1: Create the Lakehouse in this workspace\n","        create_lh_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/lakehouses\"\n","        create_lh_body = {\n","        \"displayName\": lh_name,\n","        \"description\": f\"Cloned from workspace {source_workspace_id}\"\n","        }\n","        create_lh_response = requests.post(create_lh_url, headers=headers, json=create_lh_body)\n","        lh = create_lh_response.json()\n","        \n","        flag_new = False\n","        if create_lh_response.status_code == 201: #if LH does not exist\n","                    new_lh_id = lh[\"id\"]\n","                    new_lh_name = lh[\"displayName\"]\n","                    flag_new = True\n","                    print(f\"‚úÖ Created new lakehouse: {lh_name} (ID: {new_lh_id})\")\n","        else:\n","            print(f\"‚ÑπÔ∏è Lakehouse '{lh_name}' already exists. Retrieving existing ID...\")\n","            lakehouse_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items?type=Lakehouse\"\n","            response = requests.get(lakehouse_url, headers=headers)\n","            if response.status_code == 200:\n","                lakehouses = response.json().get(\"value\", [])                \n","                for lh in lakehouses:\n","                    lh_name_aux = lh['displayName']\n","                    if lh_name == lh_name_aux : \n","                        new_lh_id = lh['id'] ##finding the lakehouse ID of the existing LH\n","                        new_lh_name = lh_name\n","        if flag_new:                    \n","                    \n","                    # -------------------------------\n","                    # Refresh SQL Endpoint\n","                    # -------------------------------\n","                    print(f\"üìÑ Refreshing sql endpoint for Lakehouse {lh_name}\")\n","                    \n","                    #Instantiate the client\n","                    client = fabric.FabricRestClient()\n","                    \n","                    sqlendpoint = None\n","                    max_attempts = 10 #  limit to avoid infinite loop\n","                    attempt = 0\n","                    while sqlendpoint is None and attempt < max_attempts:\n","                        try:\n","                            sqlendpoint = fabric.FabricRestClient().get(f\"/v1/workspaces/{workspace_id}/lakehouses/{new_lh_id}\").json()['properties']['sqlEndpointProperties']['id']\n","                            \n","                            if sqlendpoint:\n","                                print(f\"‚úÖ SQL endpoint is now available: {sqlendpoint}\")\n","                                j_son = fabric.FabricRestClient().get(f\"/v1/workspaces/{workspace_id}/lakehouses/{new_lh_id}\").json()\n","                                uri = f\"/v1/workspaces/{workspace_id}/sqlEndpoints/{sqlendpoint}/refreshMetadata?preview=true\" \n","                                payload = {} \n","                                try:\n","                                    response = client.post(uri,json= payload, lro_wait = True) \n","                                    sync_status = json.loads(response.text)\n","                                    display(sync_status)\n","                                except Exception as e: print(e)\n","                                break\n","                            else:\n","                                print(f\"‚ÑπÔ∏è Attempt {attempt + 1}: SQL endpoint not available yet. Retrying in 1 minute...\")\n","                            \n","                        except Exception as e:\n","                                print(f\"‚ÑπÔ∏èAttempt {attempt + 1}: SQL endpoint not available yet. Retrying in 1 minute...\")\n","                        time.sleep(60)\n","                        attempt += 1\n","        else:\n","            print(f\"‚ÑπÔ∏è sql endpoint for lakehouse {lh_name} already exists. No refresh required\")\n","\n","        # -------------------------------\n","        # Process Lakehouse tables\n","        # -------------------------------\n","        tables_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{source_workspace_id}/lakehouses/{lh_id}/tables\"\n","        tables_response = requests.get(tables_url, headers=headers)\n","        if tables_response.status_code == 200:\n","            \n","            tables_data = tables_response.json()\n","            tables = tables_data.get(\"data\", []) \n","            if tables:\n","                print(f\"  üìÑ Starting processing Lakehouse tables:\")\n","                for table in tables:\n","                    print(f\" Processing table - {table['name']}\")\n","                    table_name = table['name']\n","                    #get the table schema and recreate the table\n","                    path = f\"abfss://{source_workspace_id}@onelake.dfs.fabric.microsoft.com/{lh_id}/Tables/{table_name}\"                    \n","                    \n","                    df = spark.read.format(\"delta\").load(path)\n","                    schema = df.schema\n","                   # Create an empty DataFrame with the same schema\n","                    empty_df = spark.createDataFrame([], schema)\n","                    # Set the target path for the new table in the new lakehouse\n","                    target_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{new_lh_id}/Tables/{table_name}\"  # f\"Tables/{table_name}\" # Relative path inside the lakehouse\n","                    # Write the empty DataFrame as a Delta table\n","                    empty_df.write.format(\"delta\").mode(\"overwrite\").save(target_path)\n","                    \n","                    #copy storage files from source to destination\n","                    # Define source and target paths\n","                    source_path = path                    \n","                    # Clean up the destination path if it exists\n","                    if mssparkutils.fs.exists(target_path):\n","                        #print(f\"üßπ Cleaning up existing contents at: {target_path}\")\n","                        mssparkutils.fs.rm(target_path, recurse=True)\n","                    # Recreate target directory                    \n","                    mssparkutils.fs.mkdirs(target_path)\n","                    # List all files and folders in the source table directory\n","                    items = mssparkutils.fs.ls(source_path)\n","                    # Copy each item to the target path\n","                    for item in items:\n","                        source_item_path = item.path \n","                        target_item_path = f\"{target_path}/{source_item_path.split('/')[-1]}\"                     \n","                        mssparkutils.fs.cp(source_item_path, target_item_path, recurse=True)\n","                        print(f\"‚úÖ Table '{table_name}' copied successfully.\")\n","\n","                    print(f\"‚úÖ Successfully processed lakehouse: {lh_name}\")\n","                    \n","                  \n","            else:\n","                print(\"  ‚ö†Ô∏è No tables found in this lakehouse.\")\n","        else:\n","            print(f\"  ‚ùå Failed to retrieve tables: {tables_response.status_code} - {tables_response.text}\")\n","else:\n","    print(f\"‚ùå Failed to retrieve lakehouses: {response.status_code} - {response.text}\")\n","\n","print(f\"‚ÑπÔ∏è Lakehouse replication completed\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"7d830960-3bb9-4a1f-b898-dfb222c8534a","normalized_state":"finished","queued_time":"2025-07-11T10:55:56.6378749Z","session_start_time":null,"execution_start_time":"2025-07-11T10:55:56.6391875Z","execution_finish_time":"2025-07-11T10:57:43.8465812Z","parent_msg_id":"b0db393d-17b3-4baf-9356-9b63fa9c19d9"},"text/plain":"StatementMeta(, 7d830960-3bb9-4a1f-b898-dfb222c8534a, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Getting the workspace ID\nCurrent workspace ID: 2f696004-c373-434e-8cd8-4c6a1c1da359\nReplicating all lakehouses from Workspace 6d421ea5-43d4-47c6-a0f4-99eb71d8dd50 to this Workspace with id 2f696004-c373-434e-8cd8-4c6a1c1da359\nListing all Lakehouses in workspace 6d421ea5-43d4-47c6-a0f4-99eb71d8dd50\nFound 3 lakehouses in workspace 6d421ea5-43d4-47c6-a0f4-99eb71d8dd50:\n\n\nüìò Processing Lakehouse: LH_PUBLICHOLIDAYS (ID: 87943777-283d-47dd-83ce-00555a212823)\n‚ÑπÔ∏è Lakehouse 'LH_PUBLICHOLIDAYS' already exists. Retrieving existing ID...\n‚ÑπÔ∏è sql endpoint for lakehouse LH_PUBLICHOLIDAYS already exists. No refresh required\n  üìÑ Starting processing Lakehouse tables:\n Processing table - publicholidays\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/dca9f735-6fed-4ace-9422-2d534542747f/Tables/publicholidays\n‚úÖ Table 'publicholidays' copied successfully.\n‚úÖ Table 'publicholidays' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_PUBLICHOLIDAYS\n\nüìò Processing Lakehouse: LH_TAXI (ID: cb7cfd4a-f844-4092-b9ba-709252d789fc)\n‚ÑπÔ∏è Lakehouse 'LH_TAXI' already exists. Retrieving existing ID...\n‚ÑπÔ∏è sql endpoint for lakehouse LH_TAXI already exists. No refresh required\n  üìÑ Starting processing Lakehouse tables:\n Processing table - green_tripdata_2021\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/256e88f1-bc3d-4a64-8375-03d2c79aa9a9/Tables/green_tripdata_2021\n‚úÖ Table 'green_tripdata_2021' copied successfully.\n‚úÖ Table 'green_tripdata_2021' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_TAXI\n Processing table - green_tripdata_2017\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/256e88f1-bc3d-4a64-8375-03d2c79aa9a9/Tables/green_tripdata_2017\n‚úÖ Table 'green_tripdata_2017' copied successfully.\n‚úÖ Table 'green_tripdata_2017' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_TAXI\n Processing table - green_tripdata_2020\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/256e88f1-bc3d-4a64-8375-03d2c79aa9a9/Tables/green_tripdata_2020\n‚úÖ Table 'green_tripdata_2020' copied successfully.\n‚úÖ Table 'green_tripdata_2020' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_TAXI\n Processing table - green_tripdata_2019\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/256e88f1-bc3d-4a64-8375-03d2c79aa9a9/Tables/green_tripdata_2019\n‚úÖ Table 'green_tripdata_2019' copied successfully.\n‚úÖ Table 'green_tripdata_2019' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_TAXI\n Processing table - green_tripdata_2022\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/256e88f1-bc3d-4a64-8375-03d2c79aa9a9/Tables/green_tripdata_2022\n‚úÖ Table 'green_tripdata_2022' copied successfully.\n‚úÖ Table 'green_tripdata_2022' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_TAXI\n Processing table - green_tripdata_2018\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/256e88f1-bc3d-4a64-8375-03d2c79aa9a9/Tables/green_tripdata_2018\n‚úÖ Table 'green_tripdata_2018' copied successfully.\n‚úÖ Table 'green_tripdata_2018' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_TAXI\n\nüìò Processing Lakehouse: LH_RETAIL (ID: 4a83e16b-0f1b-4828-8d07-269d2a9c8805)\n‚ÑπÔ∏è Lakehouse 'LH_RETAIL' already exists. Retrieving existing ID...\n‚ÑπÔ∏è sql endpoint for lakehouse LH_RETAIL already exists. No refresh required\n  üìÑ Starting processing Lakehouse tables:\n Processing table - dimension_city\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/a2bcf370-7551-43a6-ae81-3071a3491ea3/Tables/dimension_city\n‚úÖ Table 'dimension_city' copied successfully.\n‚úÖ Table 'dimension_city' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_RETAIL\n Processing table - dimension_customer\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/a2bcf370-7551-43a6-ae81-3071a3491ea3/Tables/dimension_customer\n‚úÖ Table 'dimension_customer' copied successfully.\n‚úÖ Table 'dimension_customer' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_RETAIL\n Processing table - dimension_employee\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/a2bcf370-7551-43a6-ae81-3071a3491ea3/Tables/dimension_employee\n‚úÖ Table 'dimension_employee' copied successfully.\n‚úÖ Table 'dimension_employee' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_RETAIL\n Processing table - fact_sale\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/a2bcf370-7551-43a6-ae81-3071a3491ea3/Tables/fact_sale\n‚úÖ Table 'fact_sale' copied successfully.\n‚úÖ Table 'fact_sale' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_RETAIL\n Processing table - dimension_date\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/a2bcf370-7551-43a6-ae81-3071a3491ea3/Tables/dimension_date\n‚úÖ Table 'dimension_date' copied successfully.\n‚úÖ Table 'dimension_date' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_RETAIL\n Processing table - dimension_stock_item\nüßπ Cleaning up existing contents at: abfss://2f696004-c373-434e-8cd8-4c6a1c1da359@onelake.dfs.fabric.microsoft.com/a2bcf370-7551-43a6-ae81-3071a3491ea3/Tables/dimension_stock_item\n‚úÖ Table 'dimension_stock_item' copied successfully.\n‚úÖ Table 'dimension_stock_item' copied successfully.\n‚úÖ Successfully processed lakehouse: LH_RETAIL\n‚ÑπÔ∏è Lakehouse replication completed\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6df8c3fb-7da3-442a-809c-dbed15b1eac2"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"synapse_pyspark","language":null,"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":null}},"nbformat":4,"nbformat_minor":5}