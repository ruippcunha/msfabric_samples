{"cells":[{"cell_type":"markdown","source":["# SQL Endpoint Metadata Refresher for Lakehouses\n","# \n","This notebook allows you to refresh the SQL endpoint metadata for each lakehouse in the workspace, including listing the synchronization status of each lakehouse table.\n","\n","Author: [Rui Cunha]\n","\n","GitHub: [https://github.com/ruippcunha/msfabric_samples]\n","\n","Disclaimer\n","This notebook is provided as a sample implementation intended for development and testing purposes only. It is not designed or recommended for use in production environments. Although this notebook was developed by a Microsoft employee, it is not an officially supported solution, use it at your own risk. . Microsoft and the author assume no responsibility for any issues or data loss that may result from its use in live systems. Before applying any part of this solution in production, ensure it is thoroughly reviewed, tested, and adapted to meet your organization‚Äôs security, compliance, and operational requirements."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3348ee5c-1057-40bb-aa40-c7eebaf80240"},{"cell_type":"code","source":["!pip install semantic-link-labs --quiet"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1e03a861-76e3-4884-ba8a-3745503ae018"},{"cell_type":"code","source":["import json\n","import sempy.fabric as fabric\n","import sempy_labs as labs\n","from sempy_labs import ConnectLakehouse\n","from sempy.fabric import evaluate_measure\n","from datetime import date, datetime , time\n","import pandas as pd\n","\n","from pyspark.sql import SparkSession\n","\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n","\n","\n","import pandas as pd\n","from IPython.display import display, HTML\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7fcdb141-400f-4f5b-a4fe-0e35399f4309"},{"cell_type":"code","source":["\n","workspace_name = notebookutils.runtime.context.get(\"currentWorkspaceName\")\n","\n","print (f\"üìÑStarting the synchronization of the SQL endpoint for the lakehouses in  workspace {workspace_name}\")\n","\n","# Get today's date\n","current_date = date.today()\n","\n","# Initialize list to track lakehouses that failed validation\n","not_refreshed_endpoints = []\n","\n","# List all lakehouses in the current workspace\n","lakehouses = notebookutils.lakehouse.list()\n","\n","\n","for lakehouse in lakehouses:\n","    lakehouse_name = lakehouse.displayName\n","    lakehouse_id = lakehouse.id\n","\n","    # Only process Lakehouses explicitly listed in LAKEHOUSES parameter\n","    \n","    print(f\"‚ÑπÔ∏è Processing Lakehouse '{lakehouse_name}' (ID: {lakehouse_id})\")\n","\n","    try:\n","        with ConnectLakehouse(lakehouse=lakehouse_name, workspace=workspace_name) as sql:\n","            print(f\"‚ÑπÔ∏è Verifying that the Lakehouse SQL endpoint is active by executing a test query\")\n","            df = sql.query(\"SELECT GETDATE() AS today;\")\n","            #display(df)\n","                \n","            print(f\"‚ÑπÔ∏è Starting refreshing the {lakehouse_name} SQL endpoint metadata\")   \n","\n","            # Refresh SQL endpoint metadata\n","            client = fabric.FabricRestClient()\n","            workspace_id = fabric.get_notebook_workspace_id()\n","            sql_endpoint_id = client.get(f\"/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}\").json()['properties']['sqlEndpointProperties']['id']\n","            uri = f\"v1/workspaces/{workspace_id}/sqlEndpoints/{sql_endpoint_id}/refreshMetadata?preview=true\"\n","            response = client.post(uri, json={}, lro_wait=True)\n","            \n","            print(f\"‚ÑπÔ∏è {lakehouse_name}  SQL endpoint metadata refresh completed\")   \n","            \n","            # Convert response to JSON and pretty-print\n","            response_json = response.json()\n","            \n","            # Flatten the response\n","            flattened = []\n","            for entry in response_json:\n","                flattened.append({\n","                    \"Table Name\": entry.get(\"tableName\"),\n","                    \"Status\": entry.get(\"status\"),\n","                    \"Start Time\": entry.get(\"startDateTime\"),\n","                    \"End Time\": entry.get(\"endDateTime\"),\n","                    \"Last Successful Sync\": entry.get(\"lastSuccessfulSyncDateTime\"),\n","                    \"Error Code\": entry.get(\"error\", {}).get(\"errorCode\"),\n","                    \"Error Message\": entry.get(\"error\", {}).get(\"message\")\n","                })\n","\n","            # Create Spark session\n","            spark = SparkSession.builder.getOrCreate()\n","\n","            # Define schema\n","            schema = StructType([\n","                StructField(\"Table Name\", StringType(), True),\n","                StructField(\"Status\", StringType(), True),\n","                StructField(\"Start Time\", StringType(), True),\n","                StructField(\"End Time\", StringType(), True),\n","                StructField(\"Last Successful Sync\", StringType(), True),\n","                StructField(\"Error Code\", StringType(), True),\n","                StructField(\"Error Message\", StringType(), True)\n","            ])\n","\n","            # Create DataFrame and show\n","            df = spark.createDataFrame(flattened, schema)\n","            #df.show(truncate=False)\n","            \n","            # Convert Spark DataFrame to Pandas\n","            pandas_df = df.toPandas()\n","\n","            # Apply styling: red text for rows with an error code\n","            def highlight_errors(row):\n","                color = 'color: red;' if pd.notnull(row['Error Code']) else ''\n","                return [color] * len(row)\n","\n","            styled_df = pandas_df.style.apply(highlight_errors, axis=1)\n","\n","            # Display styled DataFrame\n","            display(HTML(styled_df.to_html()))\n","\n","\n","    except Exception as e:\n","            not_refreshed_endpoints.append(lakehouse_name)\n","            print(f\"\\‚ùå Failed to refresh or validate Lakehouse '{lakehouse_name}': {e}\")\n","\n","# Summary\n","if not_refreshed_endpoints:\n","    print(\"\\‚ùå The following lakehouses failed validation:\")\n","    for lh in not_refreshed_endpoints:\n","        print(f\"  - {lh}\")\n","else:\n","    print(\"\\‚úÖ All lakehouses validated successfully.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e3269189-cee6-4774-bf00-ba0e1245381f"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"synapse_pyspark","language":null,"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"version_major":2,"version_minor":0,"state":{"2f52acc766224d50a69252ec8b800016":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"ada54f84d3cf40d1b11e42f5ab9bcb9c":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"0ef9d52d274d439390768c5b9d0268f3":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_20b1e0fc873f4a67b81568b25d43fe35","IPY_MODEL_a772f912d9194891a1fe7476dd6b0888","IPY_MODEL_6ec102bc90f54fb0bdcbcad58a3efdfa"],"layout":"IPY_MODEL_bf0c07444e594f43b1eef4f172791b0b"}},"1b8b8e8282474bb19d75c048933ba63d":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"290d841daf424622a39081804b20b600":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"6ec102bc90f54fb0bdcbcad58a3efdfa":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 100/100 [01:03&lt;00:00,  1.66it/s]","layout":"IPY_MODEL_1b8b8e8282474bb19d75c048933ba63d","style":"IPY_MODEL_2f52acc766224d50a69252ec8b800016"}},"bf0c07444e594f43b1eef4f172791b0b":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"20b1e0fc873f4a67b81568b25d43fe35":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Operation v1/workspaces/bfdd2b55-4411-4c8c-a70d-b886df510a15/sqlEndpoints/39f92c65-f4ec-4c9f-b03e-8fcdc0f91a6e/refreshMetadata?preview=true successfully completed: 100%","layout":"IPY_MODEL_290d841daf424622a39081804b20b600","style":"IPY_MODEL_31387df0e52f4787af330dae88b34a14"}},"9812fa4e17b046d29ecae87a06ead6f4":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"a772f912d9194891a1fe7476dd6b0888":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":100,"bar_style":"success","style":"IPY_MODEL_ada54f84d3cf40d1b11e42f5ab9bcb9c","layout":"IPY_MODEL_9812fa4e17b046d29ecae87a06ead6f4"}},"31387df0e52f4787af330dae88b34a14":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}}}}},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":null,"default_lakehouse_name":"","default_lakehouse_workspace_id":"","known_lakehouses":[]}}},"nbformat":4,"nbformat_minor":5}